{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b28bf-d551-4376-9993-3a2b153d6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import array\n",
    "# import string\n",
    "# from string import punctuation\n",
    "# from itertools import chain\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, f_regression, f_classif\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "! pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# from sklearn import svm\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk import ngrams\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "! pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "! pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "! pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c5565-9194-4c63-b648-e6477b48ecc9",
   "metadata": {},
   "source": [
    "## Loading dataset and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84977e7-b8e5-4d2c-9e5d-6004a3d57641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"csv's/TrainingDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425643a1-b14e-4cab-8a47-f02b1b844275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee91202-d534-44e4-b968-a7a37f3dd146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc0f32-b37c-43c8-8cfb-0b080b399fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade3110-d1c9-4bbc-aa2b-f14d7903b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb758eaf-9d13-4cdb-8ca4-d634a0d7458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df[df['rating']>3]\n",
    "neg = df[df['rating']<3]\n",
    "print(pos.shape, neg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53742a",
   "metadata": {},
   "source": [
    "# Largely imbalanced\n",
    "May have to oversample/undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474594a-15ea-4cf7-8538-74025810ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [pos.shape[0], neg.shape[0]]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(['Positive Ratings (>3)', 'Negative Ratings (<3)'], counts, color=['green', 'red'])\n",
    "plt.title('Count of Positive and Negative Ratings')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Rating Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a1520-db64-4408-b988-f68e4b13dce3",
   "metadata": {},
   "source": [
    "## Preprocessing:\n",
    "- Done via a sentiment map\n",
    "- resulting column added to original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6fca0-371c-46dd-abd6-63c47b98ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = {1:0, 2:0, 4:1, 5:1}\n",
    "\n",
    "temp = df[df['rating'] != 3]\n",
    "y = temp['rating'].map(sentiment_map)\n",
    "\n",
    "df.loc[:, 'sentiment'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a276f-f31a-454a-bb96-1f1b125cf140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c409196-72ea-488c-869d-27d61e673dad",
   "metadata": {},
   "source": [
    "## Preparing dataframe for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd0e48-dcb7-41a5-8b7d-b5a2f36619b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_to_clean = pd.DataFrame()\n",
    "reviews_to_clean = df['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226554e2-e791-421f-b55a-ebaae203b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_to_clean.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40604f16-5a24-4554-b30b-c47b1866abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(reviews_to_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38dbf6-664e-4a2a-9329-801faf0e9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_to_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efda403-f669-4c6a-8bc6-fae65f9bc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0876e8-2920-4625-88fa-ea6a0c9c5bb7",
   "metadata": {},
   "source": [
    "## Cleaning HTML aspects\n",
    "- includes tags and https:// "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22717e14-24e3-490c-b35e-1d87d7c35a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b9e72-5b1a-48db-ba1b-0f0307f3564d",
   "metadata": {},
   "source": [
    "## Creating tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f045e-41d5-4f5e-902c-dd45dfec2d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(text):\n",
    "    if isinstance(text, str):  # If the input is a string\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token.isalpha()]  # Keep only alphabetic tokens\n",
    "        return tokens\n",
    "    elif isinstance(text, (int, float)):  # If the input is an integer or float\n",
    "        return text  # Return the number as is\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464f39f-1bed-4876-b6e9-647273442e65",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0eca34-196f-43b1-bfac-d5f3ca664fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stop_words = (stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369aa7a-c997-4cd2-b5bb-eb9f7e85842a",
   "metadata": {},
   "source": [
    "## Lemmatization:\n",
    "- Spelling or semantics of words may have been affected via tokenisation or punctuation removal. This was done to try to preserve value of those that may have been affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4fb88-ed57-46aa-b4e1-5b710697b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_text(tokens):\n",
    "#     lemma = WordNetLemmatizer()\n",
    "#     lemma_token = [lemma.lemmatize(word) for word in tokens]\n",
    "#     return lemma_token\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    if tokens is None:\n",
    "        return []  # Return an empty list if tokens is None\n",
    "    lemma = WordNetLemmatizer()\n",
    "    lemma_tokens = [lemma.lemmatize(word) for word in tokens if word is not None]  # Check for None\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77b7dd-2075-4351-8da7-73760015925e",
   "metadata": {},
   "source": [
    "## Cleaning text utilizing prior functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2fdbe-482c-41d6-aef3-7da51eb3845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Check if the input is not a string\n",
    "        return ''  # Return an empty string or handle it as needed\n",
    "    text = clean_html(text)\n",
    "    tokens = tokenize_df(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "    tokens = lemmatize_text(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8702106-cb6f-4af9-86aa-059a8723d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = reviews_to_clean.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22492b8c-6437-449d-8699-615ad1311685",
   "metadata": {},
   "source": [
    "## Convert all instances of float or integer data to string:\n",
    "- This was done to avoid instances of float or integer.\n",
    "- Chose not to drop the values since it may hold value in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c0c19-1edd-44a3-8434-79bab8b78255",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews.apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip()if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd45143-adb7-4f05-96a3-948e82f1ec0f",
   "metadata": {},
   "source": [
    "## Dropping values:\n",
    "- This was done to account for any rows that had no data. Avoids NaN representation for text_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f578aa-cddb-4c05-b03b-c6751e6e43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = cleaned_reviews.fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c20091",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews.to_csv('cleaned_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448c76f-e163-4b9f-a6a9-23f08fe5af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_reviews.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be871f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4928dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f0869-0322-4f3a-bebd-c9d3eefb0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cloud(data):\n",
    "    text = \" \".join(review for review in data)\n",
    "    print (\"There are {} words in the combination of all reviews.\".format(len(text)))\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    wordcloud = WordCloud(stopwords=stopwords_set, background_color=\"white\").generate(text)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c3eea-718b-43cc-a9e4-9ef17efb860a",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "- Small visualisation to check if cleaning went as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671602ab-df89-467b-b6eb-bc843807d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = print_cloud(cleaned_reviews)\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc6c7a-05d8-4b5a-946c-ae72d9e2e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_reviews\n",
    "print(type(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e6c23-456d-48d4-81ee-58e5fcd737c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sentiment']\n",
    "print(type(y))\n",
    "print(y.shape)\n",
    "print(y.isnull().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9011b3-0a65-4081-8194-daae1466b5b8",
   "metadata": {},
   "source": [
    "## Dropping NaN \n",
    "- Dropped those rows with NaN representations in sentiments based on the assumption their rating was a 3/5. Justified since the data loss is negligible when compared to that which remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b98846-4ad7-4433-953d-fd6944545e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.dropna()\n",
    "X = X[y.index] \n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f694c6-1289-4c62-9d8a-60c8e05dbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_fit(X, y, model, clf_model, coef_show=1):\n",
    "    X_c = model.fit_transform(X)\n",
    "    print('# features: {}'.format(X_c.shape[1]))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state=0)\n",
    "    print('# train records: {}'.format(X_train.shape[0]))\n",
    "    print('# test records: {}'.format(X_test.shape[0]))\n",
    "    clf = clf_model.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    print ('Model Recall: {}'.format(recall))\n",
    "    print ('Model F1-Score: {}'.format(f1_score(y_test,y_pred)))\n",
    "    if coef_show == 1: \n",
    "        w = model.get_feature_names_out()\n",
    "        coef = clf.coef_.tolist()[0]\n",
    "        coeff_df = pd.DataFrame({'Word' : w, 'Coefficient' : coef})\n",
    "        coeff_df = coeff_df.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "        print('')\n",
    "        print('-Top 20 positive-')\n",
    "        print(coeff_df.head(20).to_string(index=False))\n",
    "        print('')\n",
    "        print('-Top 20 negative-')        \n",
    "        print(coeff_df.tail(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448503d7-698c-4f05-8cb7-480b04f9857e",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "- A recall of approximately 0.99 indicates that the model is very effective at identifying positive sentiments in the reviews.\n",
    "- The output lists the top 20 words (or n-grams) that are most strongly associated with positive sentiment, along with their coefficients.\n",
    "- The words listed here are the strongest indicators of positive sentiment in the reviews. The coefficients represent the weight of each word in the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1861000-113d-4fa0-9e17-32bb5adf5e9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_n = TfidfVectorizer(ngram_range=(1,2), stop_words = 'english')\n",
    "text_fit(X, y, tfidf_n, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9614ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188feee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "sentiments = []\n",
    "\n",
    "reviews = X\n",
    "sentiments = y\n",
    "sentiments = np.array(list(map(lambda x: 1 if x==1 else 0, sentiments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews), len(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews, sentiments, \n",
    "                                                test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_test = word_tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2017da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = word_tokenizer.to_json()\n",
    "with io.open('embedded.json', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd829971",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6fc80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d912eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='minority', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20cf5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_flat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01871a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "X_test_resampled, y_test_resampled = smote.fit_resample(X_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ddf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d86c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(128))\n",
    "\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37680107",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_history = lstm_model.fit(X_resampled, y_resampled, batch_size=128, epochs=8, \n",
    "                                        verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fe70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lstm_model.predict(X_test_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred * 1).astype(int)\n",
    "y_pred = y_pred.flatten()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = lstm_model.evaluate(X_test_resampled, y_test_resampled, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save('lstm_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lstm_model_history.history['acc'])\n",
    "plt.plot(lstm_model_history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(lstm_model_history.history['loss'])\n",
    "plt.plot(lstm_model_history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
