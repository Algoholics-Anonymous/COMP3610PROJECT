{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bigdata_a3_utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# from utils.preprocessing import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"custom_dataset\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_csv_path = \"custom_dataset/reviews.csv\"\n",
    "metadata_csv_path = \"custom_dataset/metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ec902",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_review_df = None\n",
    "first_meta_df = None\n",
    "\n",
    "# Process each category\n",
    "for category in tqdm(VALID_CATEGORIES, desc=\"Processing categories\"):\n",
    "    print(f\"\\nProcessing category: {category}\")\n",
    "    \n",
    "    # Process review data\n",
    "    try:\n",
    "        # Use streaming mode with iterable dataset\n",
    "        review_dataset = load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\", \n",
    "            f\"raw_review_{category}\", \n",
    "            trust_remote_code=True,\n",
    "            split=\"full\",\n",
    "            streaming=True\n",
    "        )\n",
    "        \n",
    "        # Create an iterator for the dataset\n",
    "        review_iter = iter(review_dataset)\n",
    "        \n",
    "        # Collect reviews into a list (up to MAX_ROWS)\n",
    "        reviews = []\n",
    "        count = 0\n",
    "        \n",
    "        for item in review_iter:\n",
    "            if count >= MAX_ROWS:\n",
    "                break\n",
    "                \n",
    "            # Create a new dictionary instead of modifying the item in-place\n",
    "            review_item = dict(item)\n",
    "            review_item['category'] = category\n",
    "            reviews.append(review_item)\n",
    "            count += 1\n",
    "            \n",
    "            # Process in batches to conserve memory\n",
    "            if len(reviews) >= 1000 or count >= MAX_ROWS:\n",
    "                review_df = pd.DataFrame(reviews)\n",
    "                \n",
    "                # Check if this is the first batch we're writing\n",
    "                if first_review_df is None:\n",
    "                    review_df.to_csv(reviews_csv_path, mode='w', header=True, index=False)\n",
    "                    first_review_df = True\n",
    "                else:\n",
    "                    review_df.to_csv(reviews_csv_path, mode='a', header=False, index=False)\n",
    "                \n",
    "                # Clear the batch\n",
    "                reviews = []\n",
    "        \n",
    "        print(f\"  ✓ Appended {count} reviews from {category} to {reviews_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing reviews for {category}: {str(e)}\")\n",
    "    \n",
    "    # Process metadata\n",
    "    try:\n",
    "        # Use streaming mode with iterable dataset\n",
    "        meta_dataset = load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\", \n",
    "            f\"raw_meta_{category}\", \n",
    "            trust_remote_code=True,\n",
    "            split=\"full\",\n",
    "            streaming=True\n",
    "        )\n",
    "        \n",
    "        # Create an iterator for the dataset\n",
    "        meta_iter = iter(meta_dataset)\n",
    "        \n",
    "        # Collect metadata into a list (up to MAX_ROWS)\n",
    "        metadata = []\n",
    "        count = 0\n",
    "        \n",
    "        for item in meta_iter:\n",
    "            if count >= MAX_ROWS:\n",
    "                break\n",
    "                \n",
    "            # Create a new dictionary instead of modifying the item in-place\n",
    "            meta_item = dict(item)\n",
    "            meta_item['category'] = category\n",
    "            metadata.append(meta_item)\n",
    "            count += 1\n",
    "            \n",
    "            # Process in batches to conserve memory\n",
    "            if len(metadata) >= 1000 or count >= MAX_ROWS:\n",
    "                meta_df = pd.DataFrame(metadata)\n",
    "                \n",
    "                # Check if this is the first batch we're writing\n",
    "                if first_meta_df is None:\n",
    "                    meta_df.to_csv(metadata_csv_path, mode='w', header=True, index=False)\n",
    "                    first_meta_df = True\n",
    "                else:\n",
    "                    meta_df.to_csv(metadata_csv_path, mode='a', header=False, index=False)\n",
    "                \n",
    "                # Clear the batch\n",
    "                metadata = []\n",
    "        \n",
    "        print(f\"  ✓ Appended {count} metadata records from {category} to {metadata_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing metadata for {category}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nAll categories processed!\")\n",
    "print(f\"Review data saved to: {reviews_csv_path}\")\n",
    "print(f\"Metadata saved to: {metadata_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7856125",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_cache_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergedatasets():\n",
    "    reviews = pd.read_csv(\"custom_dataset/reviews.csv\")\n",
    "    metadata = pd.read_csv(\"custom_dataset/metadata.csv\")\n",
    "\n",
    "    merged = pd.merge(reviews, metadata, on= 'parent_asin', how='inner', suffixes=('_review', '_metadata'))\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.DataFrame()\n",
    "merged = mergedatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14428f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa86e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[merged['text'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ea433",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb43a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.drop_duplicates(subset=['user_id', 'asin', 'text'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(row):\n",
    "    # Check if store exists and is not empty after stripping\n",
    "    store_value = row.get('store')\n",
    "    if isinstance(store_value, str) and store_value.strip():\n",
    "        return store_value.strip()  # Return store if it has a non-empty value\n",
    "    \n",
    "    # Try to extract brand from details\n",
    "    elif row.get('details'):\n",
    "        # Use a regex pattern similar to your original function\n",
    "        match = re.search(r\"Brand[:\\s\\-]*([A-Za-z0-9&\\s]+)\", row['details'], re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()  # Return the brand found in details\n",
    "    \n",
    "    # Default fallback\n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[:, 'brand'] = merged.apply(extract_brand, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[:, 'review_length'] = merged['text'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path = \"custom_dataset/analysis.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38feef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_columns = {\n",
    "                'main_category': 'category',\n",
    "                'brand': 'brand',\n",
    "                'title_metadata': 'item',\n",
    "                'rating': 'rating',\n",
    "                'text': 'reviewText',\n",
    "                'review_length': 'text_length',\n",
    "                'timestamp': 'timestamp',\n",
    "                'rating_number': 'numRating',\n",
    "                'average_rating': 'average_rating',\n",
    "                'helpful_vote':'total_votes'\n",
    "                }\n",
    "\n",
    "merged = merged[list(needed_columns.keys())]\n",
    "merged.rename(columns=needed_columns, inplace=True)\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8078b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(analysis_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33196576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(analysis_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64772224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb6cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['category'].notnull()]\n",
    "df = df[df['item'].notnull()]\n",
    "df = df[df['reviewText'].notnull()]\n",
    "df = df[df['numRating'].notnull()]\n",
    "\n",
    "print(df.shape)\n",
    "print(df.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_to_clean = df[\"reviewText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5009072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(text):\n",
    "    if isinstance(text, str):  # If the input is a string\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token.isalpha()]  # Keep only alphabetic tokens\n",
    "        return tokens\n",
    "    elif isinstance(text, (int, float)):  # If the input is an integer or float\n",
    "        return text  # Return the number as is\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8493d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stop_words = (stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865844a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(tokens):\n",
    "    if tokens is None:\n",
    "        return []  # Return an empty list if tokens is None\n",
    "    lemma = WordNetLemmatizer()\n",
    "    lemma_tokens = [lemma.lemmatize(word) for word in tokens if word is not None]  # Check for None\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Check if the input is not a string\n",
    "        return ''  # Return an empty string or handle it as needed\n",
    "    text = clean_html(text)\n",
    "    tokens = tokenize_df(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "    tokens = lemmatize_text(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c803d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = reviews_to_clean.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = cleaned_reviews.apply(lambda x: re.sub(r\"\\s+\", \" \", str(x)).strip() if isinstance(x, (str, float, int)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'reviewText': 'cleanedText'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleanedText\"] = cleaned_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"brand\"] = df[\"brand\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb609562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"brand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86eb8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"csv's/AnalysisDataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
